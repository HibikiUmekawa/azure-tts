import os
import azure.cognitiveservices.speech as speechsdk
from dotenv import load_dotenv

# .env èª­ã¿è¾¼ã¿
load_dotenv()

speech_key = os.getenv("AZURE_SPEECH_KEY")
service_region = os.getenv("AZURE_SPEECH_REGION")

# ===================================================
# ğŸ™ï¸ æ—¥æœ¬èªéŸ³å£°ä¸€è¦§ï¼ˆå®šæ•°é…åˆ—ï¼‰
# ===================================================
JAPANESE_VOICES = [
    {"name": "ja-JP-NanamiNeural", "desc": "æ˜ã‚‹ãå…ƒæ°—ãªå¥³æ€§"},
    {"name": "ja-JP-MayuNeural", "desc": "å¹¼ã‚ã§æŸ”ã‚‰ã‹ã„å£°ï¼ˆãšã‚“ã ã‚‚ã‚“é¢¨ï¼‰"},
    {"name": "ja-JP-KeikoNeural", "desc": "ç©ã‚„ã‹ã§å„ªã—ã„å¥³æ€§"},
    {"name": "ja-JP-AoiNeural", "desc": "è½ã¡ç€ã„ãŸå¥³æ€§ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹å‘ã‘ï¼‰"},
    {"name": "ja-JP-NaokiNeural", "desc": "è‡ªç„¶ã§èãå–ã‚Šã‚„ã™ã„ç”·æ€§"},
    {"name": "ja-JP-KeitaNeural", "desc": "è‹¥ã‚ã§æ˜ã‚‹ã„ç”·æ€§"},
]

# ===================================================
# ğŸ“– text.xml ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°
# ===================================================
def read_text_file(file_path: str) -> str:
    if not os.path.exists(file_path):
        print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return ""
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read().strip()

# ===================================================
# ğŸ”§ éŸ³å£°åˆæˆé–¢æ•°
# ===================================================
def synthesize_speech(text: str, voice_index: int = 1, output_file: str = "output.wav"):
    if not text:
        print("âš ï¸ èª­ã¿ä¸Šã’ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ã€‚text.xml ã®ä¸­èº«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    if voice_index < 0 or voice_index >= len(JAPANESE_VOICES):
        print(f"âŒ ç„¡åŠ¹ãªéŸ³å£°ç•ªå·ã§ã™ã€‚0ã€œ{len(JAPANESE_VOICES)-1} ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return

    voice_info = JAPANESE_VOICES[voice_index]
    voice_name = voice_info["name"]

    print(f"ğŸ¤ é¸æŠä¸­ã®éŸ³å£°: [{voice_index}] {voice_name} ({voice_info['desc']})")

    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
    speech_config.speech_synthesis_voice_name = voice_name

    audio_config = speechsdk.audio.AudioOutputConfig(filename=output_file)
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)

    print(f"ğŸ”Š text.xml ã®å†…å®¹ã‚’éŸ³å£°ã«å¤‰æ›ä¸­...")
    result = synthesizer.speak_ssml_async(text).get()

    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        print(f"âœ… éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_file}")
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation = result.cancellation_details
        print(f"âŒ éŸ³å£°åˆæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {cancellation.reason}")
        if cancellation.reason == speechsdk.CancellationReason.Error:
            print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {cancellation.error_details}")

# ===================================================
# ğŸ§  å®Ÿè¡Œéƒ¨
# ===================================================
if __name__ == "__main__":
    print("ğŸ™ï¸ åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªéŸ³å£°:")
    for i, v in enumerate(JAPANESE_VOICES):
        print(f"  [{i}] {v['name']} - {v['desc']}")

    # idx = int(input("ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: "))
    idx = 0

    # text.xml ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€
    draft_text = read_text_file("text.xml")

    synthesize_speech(draft_text, voice_index=idx)
